{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCUUABQ0f9L//k98ksrUEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Damian0504/Unab.Pytest/blob/main/Pytest_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Hacer Solicitud HTTP\n",
        "Primero, debes hacer una solicitud HTTP para obtener el contenido de la página web que deseas raspar.\n",
        "2. Analizar el Contenido HTML Usa BeautifulSoup para analizar el HTML y extraer los datos necesarios.\n",
        "3. Interactuar con Contenido Dinámico (opcional) Si el contenido se carga dinámicamente a través de JavaScript, puedes usar Selenium para interactuar con la página.\n",
        "Respeta el archivo robots.txt:\n",
        "\n",
        "Asegúrate de que estás cumpliendo con las directrices especificadas en robots.txt del sitio web. No hagas solicitudes excesivas:\n",
        "\n",
        "Usa un retardo entre solicitudes para no sobrecargar el servidor (puedes usar time.sleep). Manejo de Errores:\n",
        "\n",
        "Implementa manejo de errores adecuado para manejar respuestas HTTP fallidas o cambios en la estructura del HTML. User-Agent:\n",
        "\n",
        "Algunos sitios bloquean solicitudes que no tienen un User-Agent adecuado. Puedes añadir un User-Agent a tus solicitudes.\n",
        "\n",
        "Ejemplo de un Web Scrapping de titulos de articulos de una pagina web.\n",
        "\n",
        "Estructuración del Código Primero, separa tu código de scraping en funciones dentro de un archivo Python, por ejemplo, scraper.py."
      ],
      "metadata": {
        "id": "lZEA8cgb398t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "url = 'https://infobae.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    page_content = response.text\n",
        "else:\n",
        "    print(f'Error: {response.status_code}')\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(page_content, 'html.parser')\n",
        "titles = soup.find_all('h2', class_='article-title')\n",
        "\n",
        "for title in titles:\n",
        "    print(title.get_text())\n",
        "\n",
        "\n",
        "options = Options()\n",
        "options.headless = True\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "driver.get(url)\n",
        "\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "titles = driver.find_elements(By.CLASS_NAME, 'article-title')\n",
        "\n",
        "for title in titles:\n",
        "    print(title.text)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "MI9aY8D44B2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de un Web Scrapping de titulos de articulos de una pagina web"
      ],
      "metadata": {
        "id": "6vj4qJfOcMSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scraper.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url, headers=None):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f'Error: {response.status_code}')\n",
        "\n",
        "def parse_titles(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    titles = soup.find_all('h2', class_='article-title')\n",
        "    return [title.get_text() for title in titles]\n",
        "\n",
        "def scrape_titles(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    page_content = fetch_page(url, headers=headers)\n",
        "    return parse_titles(page_content)\n",
        "\n",
        "\n",
        "infobae_url = 'https://infobae.com'\n",
        "titles = scrape_titles(infobae_url)\n",
        "for title in titles:\n",
        "    print(title)\n",
        ""
      ],
      "metadata": {
        "id": "eca7b3eOc9_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea un archivo de prueba, por ejemplo, test_scraper.py."
      ],
      "metadata": {
        "id": "lBwGuvHUe7Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_scraper.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url, headers):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f'Error: {response.status_code}')\n",
        "        return None\n",
        "\n",
        "def parse_titles(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    titles = soup.find_all('h2', class_='article-title')\n",
        "    return [title.get_text() for title in titles]\n",
        "\n",
        "def scrape_titles(url):\n",
        "    headers = {'User-Agent': 'Microsoft Edge'}\n",
        "    page_content = fetch_page(url, headers=headers)\n",
        "    if page_content:\n",
        "        return parse_titles(page_content)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "infobae_url = 'https://infobae.com'\n",
        "titles = scrape_titles(infobae_url)\n",
        "for title in titles:\n",
        "    print(title)"
      ],
      "metadata": {
        "id": "gC6-PPrpe8-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación de las Pruebas\n",
        "test_fetch_page:\n",
        "Verifica que el contenido de la página se obtiene correctamente.\n",
        "test_parse_titles:\n",
        "Prueba la función parse_titles con contenido HTML estático y verifica que extrae los títulos correctamente.\n",
        "test_scrape_titles:\n",
        "Combina las funciones fetch_page y parse_titles para probar el flujo completo de raspado para simular la respuesta de la solicitud HTTP."
      ],
      "metadata": {
        "id": "U0SRZqiWfant"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Herramientas Necesarias\n",
        "Para empezar con el web scraping en Python, necesitas familiarizarte con las siguientes bibliotecas:\n",
        "\n",
        "Requests: Para hacer solicitudes HTTP y obtener el contenido de las páginas web.\n",
        "BeautifulSoup: Para analizar (parsear) el HTML y extraer datos de él.\n",
        "Selenium (opcional): Para interactuar con sitios web que usan JavaScript pesado para cargar contenido dinámico.\n",
        "Instalación de Bibliotecas\n",
        "Puedes instalar estas bibliotecas usando pip:\n",
        "pip install requests\n",
        "pip install beautifulsoup4\n",
        "pip install selenium  # Solo si necesitas interactuar con JavaScript"
      ],
      "metadata": {
        "id": "vKojq7aM9xOC"
      }
    }
  ]
}