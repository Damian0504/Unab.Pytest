{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODqXUAPodno1bgQeG/AuPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Damian0504/Unab.Pytest/blob/main/Pytest_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Hacer Solicitud HTTP\n",
        "Primero, debes hacer una solicitud HTTP para obtener el contenido de la página web que deseas raspar."
      ],
      "metadata": {
        "id": "lZEA8cgb398t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    page_content = response.text\n",
        "else:\n",
        "    print(f'Error: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "MI9aY8D44B2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Analizar el Contenido HTML\n",
        "Usa BeautifulSoup para analizar el HTML y extraer los datos necesarios."
      ],
      "metadata": {
        "id": "wnxNqrEn5-_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import Beutifulsoup\n",
        "\n",
        "soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "# Encuentra los datos que necesitas, por ejemplo, todos los títulos de artículos\n",
        "titles = soup.find_all('h2', class_='article-title')\n",
        "\n",
        "for title in titles:\n",
        "    print(title.get_text())\n"
      ],
      "metadata": {
        "id": "kCPN4mDz6Epl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Interactuar con Contenido Dinámico (opcional)\n",
        "Si el contenido se carga dinámicamente a través de JavaScript, puedes usar Selenium para interactuar con la página."
      ],
      "metadata": {
        "id": "Fn2Er7uMODkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "options = Options()\n",
        "options.headless = True  # Ejecuta el navegador en modo headless (sin interfaz gráfica)\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "url = 'https://example.com'\n",
        "driver.get(url)\n",
        "\n",
        "# Espera a que el contenido cargue y luego encuentra los elementos necesarios\n",
        "titles = driver.find_elements(By.CLASS_NAME, 'article-title')\n",
        "\n",
        "for title in titles:\n",
        "    print(title.text)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "KE3SlHSrOIvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Respeta el archivo robots.txt:\n",
        "\n",
        "Asegúrate de que estás cumpliendo con las directrices especificadas en robots.txt del sitio web.\n",
        "No hagas solicitudes excesivas:\n",
        "\n",
        "Usa un retardo entre solicitudes para no sobrecargar el servidor (puedes usar time.sleep).\n",
        "Manejo de Errores:\n",
        "\n",
        "Implementa manejo de errores adecuado para manejar respuestas HTTP fallidas o cambios en la estructura del HTML.\n",
        "User-Agent:\n",
        "\n",
        "Algunos sitios bloquean solicitudes que no tienen un User-Agent adecuado. Puedes añadir un User-Agent a tus solicitudes."
      ],
      "metadata": {
        "id": "CPMyqjfzUVSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "response = requests.get(url, headers=headers)"
      ],
      "metadata": {
        "id": "zqrfuDqCUXN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de un Web Scrapping de titulos de articulos de una pagina web"
      ],
      "metadata": {
        "id": "6vj4qJfOcMSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://example.com'\n",
        "headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    page_content = response.text\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    titles = soup.find_all('h2', class_='article-title')\n",
        "\n",
        "    for title in titles:\n",
        "        print(title.get_text())\n",
        "else:\n",
        "    print(f'Error: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "wADlRqbnbeCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estructuración del Código\n",
        "Primero, separa tu código de scraping en funciones dentro de un archivo Python, por ejemplo, scraper.py."
      ],
      "metadata": {
        "id": "4bhvs0dAc4lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scraper.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page(url, headers=None):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f'Error: {response.status_code}')\n",
        "\n",
        "def parse_titles(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    titles = soup.find_all('h2', class_='article-title')\n",
        "    return [title.get_text() for title in titles]\n",
        "\n",
        "def scrape_titles(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    page_content = fetch_page(url, headers=headers)\n",
        "    return parse_titles(page_content)"
      ],
      "metadata": {
        "id": "eca7b3eOc9_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea un archivo de prueba, por ejemplo, test_scraper.py."
      ],
      "metadata": {
        "id": "lBwGuvHUe7Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_scraper.py\n",
        "import pytest\n",
        "from scraper import fetch_page, parse_titles\n",
        "\n",
        "def test_fetch_page(monkeypatch):\n",
        "    class MockResponse:\n",
        "        def __init__(self, text, status_code):\n",
        "            self.text = text\n",
        "            self.status_code = status_code\n",
        "\n",
        "    def mock_get(*args, **kwargs):\n",
        "        return MockResponse('<html><h2 class=\"article-title\">Test Title</h2></html>', 200)\n",
        "\n",
        "    monkeypatch.setattr('requests.get', mock_get)\n",
        "\n",
        "    url = 'https://example.com'\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    page_content = fetch_page(url, headers=headers)\n",
        "    assert 'Test Title' in page_content\n",
        "\n",
        "def test_parse_titles():\n",
        "    html_content = '<html><h2 class=\"article-title\">Test Title</h2></html>'\n",
        "    titles = parse_titles(html_content)\n",
        "    assert titles == ['Test Title']\n",
        "\n",
        "def test_scrape_titles(monkeypatch):\n",
        "    class MockResponse:\n",
        "        def __init__(self, text, status_code):\n",
        "            self.text = text\n",
        "            self.status_code = status_code\n",
        "\n",
        "    def mock_get(*args, **kwargs):\n",
        "        return MockResponse('<html><h2 class=\"article-title\">Test Title</h2></html>', 200)\n",
        "\n",
        "    monkeypatch.setattr('requests.get', mock_get)\n",
        "\n",
        "    url = 'https://example.com'\n",
        "    titles = scrape_titles(url)\n",
        "    assert titles == ['Test Title']"
      ],
      "metadata": {
        "id": "gC6-PPrpe8-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación de las Pruebas\n",
        "test_fetch_page:\n",
        "\n",
        "Usa monkeypatch para reemplazar la función requests.get con una versión simulada que devuelve un MockResponse.\n",
        "Verifica que el contenido de la página se obtiene correctamente.\n",
        "test_parse_titles:\n",
        "\n",
        "Prueba la función parse_titles con contenido HTML estático y verifica que extrae los títulos correctamente.\n",
        "test_scrape_titles:\n",
        "\n",
        "Combina las funciones fetch_page y parse_titles para probar el flujo completo de raspado usando monkeypatch para simular la respuesta de la solicitud HTTP."
      ],
      "metadata": {
        "id": "U0SRZqiWfant"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Herramientas Necesarias\n",
        "Para empezar con el web scraping en Python, necesitas familiarizarte con las siguientes bibliotecas:\n",
        "\n",
        "Requests: Para hacer solicitudes HTTP y obtener el contenido de las páginas web.\n",
        "BeautifulSoup: Para analizar (parsear) el HTML y extraer datos de él.\n",
        "Selenium (opcional): Para interactuar con sitios web que usan JavaScript pesado para cargar contenido dinámico.\n",
        "Instalación de Bibliotecas\n",
        "Puedes instalar estas bibliotecas usando pip:\n",
        "pip install requests\n",
        "pip install beautifulsoup4\n",
        "pip install selenium  # Solo si necesitas interactuar con JavaScript"
      ],
      "metadata": {
        "id": "vKojq7aM9xOC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AE-n-wV7-CQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}