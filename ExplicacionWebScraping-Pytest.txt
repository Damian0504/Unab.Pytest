#WebScraping_Pytest.py

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

url = 'https://infobae.com'
response = requests.get(url)

if response.status_code == 200:
    page_content = response.text
else:
    print(f'Error: {response.status_code}')

soup = BeautifulSoup(page_content, 'html.parser')
titles = soup.find_all('h2', class_='article-title')

for title in titles:
    print(title.get_text())

options = Options()
options.headless = True
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

driver.get(url)

driver.implicitly_wait(10)  # Espera 10 segundos

titles = driver.find_elements(By.CLASS_NAME, 'article-title')

for title in titles:
    print(title.text)

driver.quit()

#Scraping.py

import requests
from bs4 import BeautifulSoup

def fetch_page(url, headers=None, requests_module=requests):
    response = requests_module.get(url, headers=headers)
    if response.status_code == 200:
        return response.text
    else:
        raise Exception(f'Error: {response.status_code}')

def parse_titles(page_content):
    soup = BeautifulSoup(page_content, 'html.parser')
    titles = soup.find_all('h2', class_='article-title')
    return [title.get_text() for title in titles]

def scrape_titles(url, requests_module=requests):
    headers = {'User-Agent': 'GoogleChrome'}
    page_content = fetch_page(url, headers=headers, requests_module=requests_module)
    return parse_titles(page_content)

#Test_scraping.py

import pytest
import requests
from bs4 import BeautifulSoup
from my_scraping_module import fetch_page, parse_titles, scrape_titles 

def test_fetch_page_success(requests_mock):
    url = 'https://infobae.com'
    headers = {'User-Agent': 'GoogleChrome'}
    mock_html = '<html><body><h2 class="article-title">Test Title</h2></body></html>'
    requests_mock.get(url, text=mock_html)

    result = fetch_page(url, headers)
    assert result == mock_html

def test_fetch_page_error(requests_mock):
    url = 'https://infobae.com'
    headers = {'User-Agent': 'GoogleChrome'}
    requests_mock.get(url, status_code=404)

    with pytest.raises(Exception, match="Error: 404"):
        fetch_page(url, headers)

def test_parse_titles():
    html_content = '<html><body><h2 class="article-title">Test Title 1</h2><h2 class="article-title">Test Title 2</h2></body></html>'
    expected_titles = ['Test Title 1', 'Test Title 2']

    titles = parse_titles(html_content)
    assert titles == expected_titles

def test_scrape_titles(requests_mock):
    url = 'https://infobae.com'
    headers = {'User-Agent': 'GoogleChrome'}
    mock_html = '<html><body><h2 class="article-title">Test Title</h2></body></html>'
    requests_mock.get(url, text=mock_html)

    expected_titles = ['Test Title']
    titles = scrape_titles(url)
    assert titles == expected_titles

Explicacion del codigo WebScraping_Pytest.py

Requests: Biblioteca para realizar solicitudes HTTP en Python.
BeautifulSoup: Biblioteca para analizar documentos HTML y XML.
Selenium.webdriver: Herramienta para automatizar navegadores web.
By: Clase que permite especificar métodos de localización de elementos en Selenium.
Service: Clase que permite manejar el servicio de ChromeDriver.
Options: Clase que permite configurar opciones de ChromeDriver.
Chrome DriverManager: Utilidad que gestiona la instalación del ChromeDriver.
Url: Define la URL de la página web de Infobae.
Requests.get(url): Realiza una solicitud GET a la URL.
if response.status_code == 200: Verifica si la solicitud fue exitosa (código de estado 200).
Page_content = response.text: Almacena el contenido HTML de la página si la solicitud fue exitosa.
Else: Imprime un mensaje de error si la solicitud no fue exitosa.
Soup = BeautifulSoup(page_content, 'html.parser'): Analiza el contenido HTML utilizando BeautifulSoup.
Titles = soup.find_all('h2', class_='article-title'): Encuentra todos los elementos <h2> con la clase article-title.
For title in titles: Itera sobre los elementos encontrados.
Print(title.get_text()): Imprime el texto de cada título.
Options = Options(): Crea un objeto de opciones para Chrome.
Options.headless = True: Configura el navegador para que se ejecute en modo headless (sin interfaz gráfica).
Driver=webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options): Configura y lanza una instancia de ChromeDriver utilizando webdriver_manager para manejar la instalación del driver.
Driver.get(url): Navega a la URL especificada.
Driver.implicitly_wait(10): Espera hasta 10 segundos para que los elementos se carguen antes de lanzar una excepción.
Titles = driver.find_elements(By.CLASS_NAME, 'article-title'): Encuentra todos los elementos con la clase article-title en la página.
For title in titles: Itera sobre los elementos encontrados.
Print(title.text): Imprime el texto de cada título.
Driver.quit(): Cierra el navegador y termina la sesión de WebDriver.

Resumen
Este código combina dos métodos diferentes para hacer web scraping:
Usando requests y BeautifulSoup:
Realiza una solicitud HTTP para obtener el contenido HTML de la página.
Analiza el HTML y extrae los títulos de los artículos.
Usando Selenium:
Automatiza un navegador web (Chrome en este caso) para cargar la página.
Extrae los títulos de los artículos utilizando métodos de localización de Selenium.
Esto puede ser útil si la página tiene contenido dinámico cargado por JavaScript, que no sería visible con solo requests y BeautifulSoup

#Explicacion del codigo Scraping.py

Función fetch_page
Propósito: Realiza una solicitud HTTP GET a una URL especificada y devuelve el contenido HTML de la página si la solicitud es exitosa.
Parámetros:
Url: La URL de la página web que se quiere recuperar.
Headers: Un diccionario de encabezados HTTP para enviar con la solicitud.
Funcionamiento:
Realiza una solicitud GET a la URL usando requests.get.
Si la solicitud es exitosa (código de estado 200), devuelve el contenido HTML de la página (response.text).
Si la solicitud falla (cualquier código de estado diferente de 200), lanza una excepción con un mensaje de error e imprime un mensaje de error y devuelve None.

Función parse_titles
Propósito: Analiza el contenido HTML para extraer los títulos de los artículos.
Parámetros: html_content: El contenido HTML de la página web.
Funcionamiento: Crea un objeto BeautifulSoup para analizar el contenido HTML.
Busca todos los elementos <h2> con la clase article-title.
Extrae el texto de cada elemento encontrado y devuelve una lista de estos textos.

Función scrape_titles
Propósito: Orquestar el proceso de recuperación y análisis de títulos de artículos desde una URL específica.
Parámetros: La URL de la página web que se quiere analizar.
Funcionamiento: Define un diccionario de encabezados con un agente de usuario ('User-Agent': 'GoogleChrome').
Llama a fetch_page para obtener el contenido HTML de la página.
Si fetch_page devuelve contenido HTML válido:
Llama a parse_titles para extraer los títulos de los artículos.
Devuelve la lista de títulos.
Si fetch_page devuelve None (scraping.py):
Devuelve una lista vacía.

#Explicacion del codigo: Test_scraping.py

Beautifulsoup: Se importa BeautifulSoup desde el módulo bs4 para extraer datos de una pagina web y contenidos HTML.
Pytest: Biblioteca utilizada para escribir y ejecutar pruebas.

Test_fetch_page_success:
Simula una respuesta exitosa con contenido HTML.
Verifica que fetch_page devuelva el contenido HTML correcto.

Test_fetch_page_error:
Simula una respuesta con un error 404.
Verifica que fetch_page lance una excepción con el mensaje de error correcto.

Test_parse_titles:
Proporciona contenido HTML directamente a parse_titles.
Verifica que parse_titles extraiga y devuelva los títulos correctos.

Test_scrape_titles:
Simula una respuesta exitosa con contenido HTML.

 
Ejecutar las Pruebas:
Para ejecutar estas pruebas, asegúrate de tener pytest instalado y luego ejecuta pytest en la línea de comandos en el directorio donde se encuentra tu archivo de pruebas.
pytest descubrirá automáticamente las funciones que comienzan con test y ejecutará las pruebas, proporcionando un reporte de los resultados.

Este conjunto de pruebas garantiza que las funciones fetch_page_success, fetch_ page, parse_titles y scrape_titles funcionan correctamente bajo condiciones controladas.
